<!doctype html>
<html lang="en" class="no-js">

<head>
    <!-- Jekyll Ideal Image Slider Include -->
    <!-- https://github.com/jekylltools/jekyll-ideal-image-slider-include -->
    <!-- v1.8 -->
    <meta charset="utf-8"> <!-- begin SEO -->
    <title>CCTV-Pipe Dataset - VideoPipe</title>
    <meta property="og:locale" content="en-US">
    <meta property="og:site_name" content="VideoPipe">
    <meta property="og:title" content="CCTV-Pipe Dataset">
    <link rel="canonical" href="index.html">
    <meta property="og:url" content="https://videopipe.github.io/cctvpipe/">
    <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "Person",
            "name": "VideoPipe",
            "url": "https://videopipe.github.io",
            "sameAs": null
        }
    </script> <!-- end SEO -->
    <link href="../feed.xml" type="application/atom+xml" rel="alternate" title="VideoPipe Feed">
    <!-- http://t.co/dKP3o1e -->
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script>
        document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
    </script> <!-- For all browsers -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <meta http-equiv="cleartype" content="on"> <!-- start custom head snippets -->
    
    <link rel="icon" type="image/png" href="../images/favicon-32x32.png" sizes="32x32">
    <!-- <link rel="icon" type="image/png" href="https://videopipe.github.io/images/android-chrome-192x192.png"
        sizes="192x192"> -->
    <link rel="icon" type="image/png" href="../images/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="../images/favicon-16x16.png" sizes="16x16">
    <link rel="manifest" href="../images/manifest.json">
    <link rel="mask-icon" href="../images/safari-pinned-tab.svg" color="#000000">
    <link rel="shortcut icon" href="../images/favicon.ico">
    <meta name="msapplication-TileColor" content="#000000">
    <meta name="msapplication-TileImage" content="https://videopipe.github.io/images/mstile-144x144.png">
    <meta name="msapplication-config" content="https://videopipe.github.io/images/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">
    <link rel="stylesheet" href="../assets/css/academicons.css" />
    <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); 
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } }); </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>
    <!-- end custom head snippets -->
</head>

<body>
    <!--[if lt IE 9]><div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->
    <!-- 头部内容 -->
    <div style="background-image: url('images/top_no_text.png'); background-size: 100% 100%; height:200px; border=20px; min-width: 760px;"
        class="center">
        <div style="background-color: rgba(256,256,256,0.6); width: 100%; height: 100%; min-width: 760px;">
            <div style="width: 96%; height: 96%; margin: 2%; min-width: 760px" class="center">
                <p style="color:rgb(73, 78, 82); font-size: 28px; margin: 0px;"><b>VideoPipe@ ICPR2022</b></p>
                <p style="color:rgb(73, 78, 82); font-size: 32px; text-align:center; margin: 0px;"><b>Challenge on VideoPipe
                    <br>Real-World Video Understanding for Urban Pipe Inspection</b></p>
                <p style="color:rgb(73, 78, 82); font-size: 28px; margin: 0px; text-align: right;"><b>
                        Sunday 21th August 2022</b></p>
            </div>
        </div>
    </div>
    <style>
        .dropdown {
            position: relative;
            display: inline-block;
        }

        .dropdown-content {
            display: none;
            position: absolute;
            background-color: #f9f9f9;
            box-shadow: 0px 8px 16px 0px rgba(0, 0, 0, 0.2);
        }

        .dropdown:hover .dropdown-content {
            display: block;
        }
    </style>
    <div class="masthead">
        <div class="masthead__inner-wrap">
            <!-- 菜单 -->
            <div class="masthead__menu">
                <nav id="site-nav" class="greedy-nav"> <button>
                        <div class="navicon"></div>
                    </button>
                    <ul class="visible-links clearfix">
                        <!-- 补充logo -->
                        <li class="masthead__menu-item masthead__menu-item--lg"><a href="../index.html">
                                <image src="../images/logo.png" style="height:54px">
                            </a></li>
                        <li class="masthead__menu-item">
                            <div class="dropdown"> <span><a href="../index.html">Datasets</a></span>
                                <div class="dropdown-content">
                                    <p><a href="../qvpipe/index.html">QV-Pipe</a></p>
                                    <p><a href="../cctvpipe/index.html">CCTV-Pipe</a></p>
                                    <!-- <p><a href="kineticstps/index.html">Kinetics-TPS</a></p> -->
                                </div>
                            </div>
                        </li>
                        <li class="masthead__menu-item"><a href="../tracks/index.html">Tracks</a></li>
                        <li class="masthead__menu-item"><a href="../results/index.html">Challenge Results</a></li>
                        <!-- <li class="masthead__menu-item"><a href="importantdates/index.html">Important Dates</a></li> -->
                        <li class="masthead__menu-item"><a href="../organizers/index.html">Organizers</a></li>
                        <!-- <li class="masthead__menu-item"><a href="../speakers/index.html">Invited Speakers</a></li> -->
                        <li class="masthead__menu-item"><a href="../program/index.html">Program Schedule</a></li>
                    </ul>
                    <ul class="hidden-links hidden"></ul>
                </nav>
            </div><!-- 菜单 end -->
        </div>
    </div>
    <div id="main" role="main">
        <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
            <meta itemprop="headline" content="CCTV-Pipe Dataset">
            <div class="page__inner-wrap">
                <header>
                    <h1 class="page__title" itemprop="headline">CCTV-Pipe Dataset</h1>
                </header>
                <section class="page__content" itemprop="text">
                    <!-- Main -->
                    <!-- <p style="margin-top:3px; margin-bottom:12px"> FineAction: A Fine-Grained Video Dataset for Temporal
                        Action Localization<br> [<a href="https://arxiv.org/abs/2105.11107">paper</a>]</p>
                    <p align="center" style="margin-top:12px"> <a href="mailto:yi.liu1@siat.ac.cn"><i
                                class="fas fa-envelope-open-text"></i></a>&nbsp <a href="https://www.yiliu.me/">Yi
                            Liu</a>&nbsp&nbsp <a href="mailto:lmwang.nju@gmail.com"><i
                                class="fas fa-envelope-open-text"></i></a>&nbsp <a
                            href="http://wanglimin.github.io">Limin Wang</a>&nbsp&nbsp <a
                            href="mailto:xiao.ma@siat.ac.cn"><i class="fas fa-envelope-open-text"></i></a>&nbsp <a
                            href=" https://hypnosx.github.io/Atopos.github.io">Xiao Ma</a>&nbsp&nbsp <a
                            href="mailto:yl.wang@siat.ac.cn"><i class="fas fa-envelope-open-text"></i></a>&nbsp <a
                            href="https://scholar.google.com/citations?hl=zh-CN&user=hD948dkAAAAJ">Yali
                            Wang</a>&nbsp&nbsp <a href="mailto:yu.qiao@siat.ac.cn"><i
                                class="fas fa-envelope-open-text"></i></a>&nbsp <a
                            href="http://mmlab.siat.ac.cn/yuqiao">Yu Qiao</a>&nbsp&nbsp</p>
                    <p align="center"> <a href="http://mmlab.siat.ac.cn">MMLAB @ Shenzhen Institute of Advanced
                            Technology</a></p>
                    <p align="center"> <a href="http://mcg.nju.edu.cn/en/index.html">MCG Group @ Nanjing University</a>
                    </p> -->
                    <div class="box">
                        <!-- <center> <span class="image fit"> <img src='../images/QV-Pipe Dataset.png'> </span> </center> -->
                        <!-- <span class="image fit"> <img src="/images/fineaction/show1.png" alt="" /> </span> <span class="image fit"> <img src="/images/fineaction/show2.png" alt="" /> </span> -->
                        <!--<p style="text-align:justify; text-justify:inter-ideograph;"><small>The 25fps tubelets of bounding boxes and fine-grained action category annotations in the sample frames of <i>MultiSports</i> dataset. Multiple concurrent action situations frequently appear in <i>MultiSports</i> with many starting and ending points in the long untrimmed video clips. The frames are cropped and sampled by stride 5 or 7 for visualization propose. Tubes with the same color represent the same person.</small></p>-->
                        <hr />
                        <header>
                            <h3>Abstract</h3>
                        </header>
                        <p style="text-align:justify; text-justify:inter-ideograph;">We have carefully collected and annotated 
                            two new industrial video datasets, namely QV-Pipe and CCTV-Pipe, for video understanding in urban pipe 
                            inspection. Specifically,<b><i> QV-Pipe is used for video defect classification (Task1) and CCTV-Pipe 
                            is used for temporal defect localization (Task 2).</i></b></p>
                        <p style="text-align:justify; text-justify:inter-ideograph;">Note that, all the participants are required 
                            to sign a copyright form for academic research, before getting our datasets. Besides, the datasets are 
                            based on the real-world pipe networks. Hence, we have deleted the information of street, city 
                            and any other about privacy in our datasets.</p>
                        <!--<header><h3>Demo Video</h3><p>Please choose "1080P" for better experience.</p></header><p align="center"><iframe width="600" height="320" src="https://www.youtube.com/embed/uGjvKYWZ5Ww" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>-->
                        <!--<header><h3>Hierarchy of Action Category</h3></header><span class="image fit"><img src="/images/ms_hier.png" alt=""/></span><p style="text-align:justify; text-justify:inter-ideograph;"> The action vocabulary hierarchy and annotator interface of the <i>MultiSports</i> dataset. Our <i>MultiSports</i> has a two-level hierarchy of action vocabularies, where the actions of each sport are fine-grained.</p>-->
                        <header>
                            <h3>Data Collection & Annotation</h3>
                        </header>
                        <p style="text-align:justify; text-justify:inter-ideograph;">Our CCTV-Pipe dataset consists of 16 defect 
                            categories including structural and functional defects in the pipe. It contains 576 videos with 89 hours, 
                            which are collected from real-world urban pipe systems. Different from traditional temporal action 
                            localization, our goal in this realistic scenario is to find preferable temporal locations of defects 
                            from a untrimmed CCTV video, instead of exact temporal boundaries. Hence, the professional engineers 
                            are asked to annotate a single frame for each defect. The annotation procedure has been checked 
                            multiple rounds with cross validation, to guarantee label quality.</p>
                        <!-- <ol>
                            <li> Complete organizational taxonomy behind FineAction. <span class="image fit"><img
                                        src="../images/fineaction/category.png" /></span></li>
                        </ol> -->
                        <header>
                            <h3>Data Comparison</h3>
                        </header>
                        <p style="text-align:justify; text-justify:inter-ideograph;"> We show some examples of CCTV-Pipe in Figure 1. 
                            We can see that, several defects appear at the same temporal location. Additionally, as demonstrated in 
                            Figure 2, the number of defects in each category ranges from 8 to 2,770. Such long-tailed distribution 
                            also raises new challenges for temporal defect localization.</p>
                        <center> <span class="image fit"> <img src='../images/CCTV-Pipe Dataset.png'> </span> 
                            <p>Figure 1. Examples of Our CCTV-Pipe Dataset. (ML: Multi-Labeled)</p></center>
                        <center> <span class="image fit"> <img src='../images/Data Distribution of CCTV-Pipe.png'> </span> 
                                <p>Figure 2. Data Distribution of CCTV-Pipe</p></center>
                        
                        <p style="text-align:justify; text-justify:inter-ideograph;"> Moreover, we compare it with the existing video 
                            benchmarks in temporal localization. As shown in Table 1, our CCTV-Pipe dataset shows the following distinct 
                            characteristics. First, compared to the existing benchmarks, videos in our CCTV-Pipe can be very long in 
                            practice, e.g., average video duration is 562 s. It is quite challenging to find temporal locations of 
                            pipe defect from such long untrimmed videos. Second, instead of traditional segment annotation, we adopt 
                            single-frame annotation for realistic demand in urban pipe inspection. Moreover, multiple defects can 
                            densely appear at the same temporal location. These facts make our CCTV-Pipe as a challenging dataset 
                            for temporal localization.</p>
                        <center><p>Table 1. Temporal Localization Benchmark Comparison</p>
                             <span class="image fit"> <img src='../images/Temporal Localization Benchmark Comparison.png'> </span> 
                            </center>

                        <p style="text-align:justify; text-justify:inter-ideograph;"> Finally, we compare it with the existing benchmarks 
                            in pipe defect inspection. As shown in Table 2, our dataset is based on videos, which is closer to urban pipe 
                            inspection in the real scenes. Moreover, our dataset is much larger than the existing ones, which opens new 
                            opportunities to develop powerful models for automatic defect inspection of urban pipe systems.</p>
                        <center> <p>Table 2. Urban Pipe Inspection Dataset Comparison</p>
                            <span class="image fit"> <img src='../images/Urban Pipe Inspection Dataset Comparison.png'> </span> 
                            </center>

                        <!-- <ol>
                            <li> Comparison with Related Benchmarks. Our FineAction is unique due to its fine-grained
                                action classes, multi-label and dense annotations, relatively large-scale capacity, and
                                rich action diversity. <span class="image fit"><img
                                        src="../images/fineaction/tab1.png" /></span></li>
                            <li> Number of instances per category. We plot the instance distribution of all the
                                bottom-level categories in each top-level category. All the plots exhibit the natural
                                long-tailed distribution. <span class="image fit"><img
                                        src="../images/fineaction/fig1.png" /></span></li>
                        </ol> -->
                        <!--<header><h3>Dataset Properties</h3></header><p style="text-align:justify; text-justify:inter-ideograph;"> Our <i>MultiSports</i> contains 66 fine-grained action categories from four different sports, selected from 247 competition records. The records are manually cut into 800 clips per sport to keep the balance of data size between sports, where we discard intervals with only background scenes, such as award, and select the highlights of competitions as video clips for action localization.</p><ol><li> Overall comparison of statistics between existing action localization datasets and our <i>MultiSports</i> v1.0. (* only train and val sets' ground-truths are available, † number of person tracklets, each of which has one or more action labels, ‡ 1fps action annotations have no clear action boundaries) <span class="image fit"><img src="/images/fineaction/tab1.png" alt="comparison with other dataset"/></span></li><li> Overall comparison of statistics between existing action localization datasets and our <i>MultiSports</i> v1.0. (* only train and val sets' ground-truths are available, † number of person tracklets, each of which has one or more action labels, ‡ 1fps action annotations have no clear action boundaries) <span class="image fit"><img src="/images/fineaction/pic1.png" alt="comparison with other dataset"/></span></li><li> Statistics of each action class's data size in <i>MultiSports</i> sorted by descending order with 4 colors indicating 4 different sports. For actions in the different sports sharing the same name, we add the name of sports after them. The natural long-tailed distribution of action categories raises new challenges for action localization models. <span class="image fit"><img src="/images/fineaction/pic2.png" alt="number of instances"/></span></li></ol>-->
                        <!-- <header>
                            <h3>Experiment Results</h3>
                        </header>
                        <ol>
                            <li> Comparison of the state-of-the-art methods on the validation set of FineAction. Left:
                                evaluation on action proposal generation, in terms of AR@AN. Right : evaluation on
                                action detection, in terms of mAP at IoU thresholds from 0.5 to 0.95. <span
                                    class="image fit"><img src="../images/fineaction/exp1.png" /></span></li>
                            <li> Error analysis on FineAction. (a) Left : the error distribution over the number of
                                predictions per video. G means the number of Ground-Truth instances. Right: the impact
                                of error types, measured by the improvement gained from resolving a particular type of
                                error. (b) Visualization of typical failure cases on FineAction. <span
                                    class="image fit"><img src="../images/fineaction/exp2.png" /></span></li>
                        </ol> -->
                        <header>
                            <h3>Download</h3>
                            <p>Please refer to the <a href="#">competition page</a> for more information.</p>
                            <!-- <p>Please refer to the <a
                                    href="https://competitions.codalab.org/competitions/32363">competition page</a> for
                                more information.</p> -->
                            <!--<p > <a href="https://pan.baidu.com/s/1LiCXqqhsJAOf05oyOh_h6g">Raw_video</a> Access Code: e0e2 <br></p><p > <a href="https://pan.baidu.com/s/11DGhaS5Agtk9Ma_tSo9TaA">i3d feature</a> Access Code: bl7y <br></p><p > <a href="https://pan.baidu.com/s/15JONgemgCKa2Y8747wAVoA">i3d feature-100</a> Access Code: w0rx <br></p>-->
                        </header>
                        <header>
                            <h3>Reference</h3>
                        </header>
                        <ul>
                            <li style="text-align:justify; text-justify:inter-ideograph;">[1] Idrees H, Zamir A R, Jiang Y G, et al. 
                                The THUMOS challenge on action recognition for videos “in the wild”. Computer Vision and Image Understanding, 155, 2017.</li>
                            <li style="text-align:justify; text-justify:inter-ideograph;">[2] Caba Heilbron F, Escorcia V, Ghanem B, et al. 
                                Activitynet: A large-scale video benchmark for human activity understanding. 
                                IEEE conference on computer vision and pattern recognition. 2015.</li>
                            <li style="text-align:justify; text-justify:inter-ideograph;">[3] Zhao H, Torralba A, Torresani L, et al. 
                                Hacs: Human action clips and segments dataset for recognition and temporal localization. 
                                IEEE/CVF International Conference on Computer Vision. 2019.</li>
                            <li style="text-align:justify; text-justify:inter-ideograph;">[4] Xiangyang Ye, Jian’e Zuo, Ruohan Li, Yajiao Wang, 
                                Lili Gan, Zhonghan Yu, and Xiaoqing Hu. Diagnosis of sewer pipe defects on image recognition of 
                                multi-features and support vector machine in a southern chinese city. Frontiers of Environmental 
                                Science & Engineering, 13(2), 2019.</li>
                            <li style="text-align:justify; text-justify:inter-ideograph;">[5] Joshua Myrans, Richard Everson, and Zoran Kapelan. 
                                Automated detection of fault types in cctv sewer surveys. Journal of Hydroinformatics, 21(1):153–163, 2018.</li>
                            <li style="text-align:justify; text-justify:inter-ideograph;">[6] Kefan Chen, Hong Hu, Chaozhan Chen, Long Chen, 
                                and Caiying He. An intelligent sewer defect detection method based on convolutional neural network. 
                                IEEE International Conference on Information and Automation, 2018.</li>
                            <li style="text-align:justify; text-justify:inter-ideograph;">[7] Duanshun Li, Anran Cong, and Shuai Guo. 
                                Sewer damage detection from imbalanced cctv inspection data using deep convolutional neural networks with hierarchical 
                                classification. Automation in Construction, 101, 2019.</li>
                            <li style="text-align:justify; text-justify:inter-ideograph;">[8] Srinath S. Kumar, Dulcy M. Abraham, Mohammad R. Jahanshahi, 
                                Tom Iseley, and Justin Starr. Automated defect classification in sewer closed circuit television 
                                inspections using deep convolutional neural networks. Automation in Construction, 91, 2018.</li>
                            <li style="text-align:justify; text-justify:inter-ideograph;">[9] Dirk Meijer, Lisa Scholten, Francois Clemens, 
                                and Arno Knobbe. A defect classification methodology for sewer image sets with convolutional neural networks. 
                                Automation in Construction, 104, 2019.</li>
                            <li style="text-align:justify; text-justify:inter-ideograph;">[10] Qian Xie, Dawei Li, Jinxuan Xu, Zhenghao Yu, 
                                and Jun Wang. Automatic detection and classification of sewer defects via hierarchical deep learning. 
                                IEEE Transactions on Automation Science and Engineering, 2019.</li>
                            <li style="text-align:justify; text-justify:inter-ideograph;">[11] Syed Ibrahim Hassan, L. Minh Dang, Irfan Mehmood, 
                                Suhyeon Im, Changho Choi, Jaemo Kang, Young-Soo Park, and Hyeonjoon Moon. Underground sewer pipe 
                                condition assessment based on convolutional neural networks. Automation in Construction, 106, 2019.</li>
                            <li style="text-align:justify; text-justify:inter-ideograph;">[12] Haurum J B, Moeslund T B. Sewer-ML: A Multi-Label 
                                Sewer Defect Classification Dataset and Benchmark. IEEE/CVF Conference on Computer Vision and Pattern 
                                Recognition. 2021: 13456-13467.</li>
                        </ul>
                    </div>
                </section>
                <footer class="page__meta"></footer>
            </div>
        </article>
    </div>
    <div class="page__footer">
        <footer>
            <!-- start custom footer snippets -->
            <!-- end custom footer snippets -->
            <div class="page__footer-follow">
                <ul class="social-icons"></ul>
            </div>
            <div class="page__footer-copyright">&copy; 2021 VideoPipe. Powered by <a href="http://jekyllrb.com"
                    rel="nofollow">Jekyll</a> &amp; <a
                    href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a
                    href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal
                    Mistakes</a>.</div>
        </footer>
    </div>
    <script src="../assets/js/main.min.js"></script>
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o), m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
        ga('create', '', 'auto');
        ga('send', 'pageview');
    </script> <!-- Jekyll Ideal Image Slider Include -->
    <!-- https://github.com/jekylltools/jekyll-ideal-image-slider-include -->
    <!-- v1.8 -->
</body>

</html>